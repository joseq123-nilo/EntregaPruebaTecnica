{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8IoG3oduzqc",
        "outputId": "46ac79a4-3f27-4642-9a9d-27c0c43f33ec"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.5)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"ETL_Films\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "RZy00wFUvug4"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc = spark.sparkContext"
      ],
      "metadata": {
        "id": "LESAq7NlwXWf"
      },
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CARGA DE DATOS**"
      ],
      "metadata": {
        "id": "4VsDoL71xqSV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#CARGA DE TABLAS\n",
        "\n",
        "df_film = spark.read.csv(\"Films.csv\", header=True, sep=\";\")\n",
        "df_customer = spark.read.csv(\"customer.csv\", header=True, sep=\";\")\n",
        "df_inventory = spark.read.csv(\"inventory.csv\", header=True,sep=\";\")\n",
        "df_rental = spark.read.csv(\"rental.csv\", header=True, sep=\";\")\n",
        "df_store = spark.read.csv(\"store.csv\", header=True, sep=\";\")"
      ],
      "metadata": {
        "id": "y1wTakD7uewi"
      },
      "execution_count": 204,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inspeccion_inicial(df, nombre_df):\n",
        "    print(f\"\\n=== Inspección del DataFrame: {nombre_df} ===\")\n",
        "    print(\"Esquema del DataFrame:\")\n",
        "    df.printSchema()\n",
        "\n",
        "    print(\"\\nPrimeras 5 filas:\")\n",
        "    df.show(5, truncate=False)\n",
        "\n",
        "    print(\"\\nNúmero total de registros:\")\n",
        "    print(df.count())\n",
        "\n",
        "    print(\"\\nNombres de columnas:\")\n",
        "    print(df.columns)"
      ],
      "metadata": {
        "id": "I8m4fPEemwu0"
      },
      "execution_count": 321,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inspeccion_inicial(df_film, \"df_film\")\n",
        "inspeccion_inicial(df_customer, \"df_customer\")\n",
        "inspeccion_inicial(df_inventory, \"df_inventory\")\n",
        "inspeccion_inicial(df_rental, \"df_rental\")\n",
        "inspeccion_inicial(df_store, \"df_store\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8RzD2FanHeB",
        "outputId": "9b92cfd2-a945-4ab7-9629-dc7f64fef46e"
      },
      "execution_count": 322,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Inspección del DataFrame: df_film ===\n",
            "Esquema del DataFrame:\n",
            "root\n",
            " |-- film_id: string (nullable = true)\n",
            " |--  title: string (nullable = true)\n",
            " |--  description: string (nullable = true)\n",
            " |--  release_year: string (nullable = true)\n",
            " |--  language_id: string (nullable = true)\n",
            " |--  original_language_id: string (nullable = true)\n",
            " |--  rental_duration: string (nullable = true)\n",
            " |--  rental_rate: string (nullable = true)\n",
            " |--  length: string (nullable = true)\n",
            " |--  replacement_cost: string (nullable = true)\n",
            " |--  num_voted_users: string (nullable = true)\n",
            " |--  rating: string (nullable = true)\n",
            " |--  special_features: string (nullable = true)\n",
            " |-- last_update: string (nullable = true)\n",
            "\n",
            "\n",
            "Primeras 5 filas:\n",
            "+-------+-----------------+----------------------------------------------------------------------------------------------------------------------+-------------+------------+---------------------+----------------+------------+-------+-----------------+----------------+-------+-----------------+--------------------+\n",
            "|film_id| title           | description                                                                                                          | release_year| language_id| original_language_id| rental_duration| rental_rate| length| replacement_cost| num_voted_users| rating| special_features|last_update         |\n",
            "+-------+-----------------+----------------------------------------------------------------------------------------------------------------------+-------------+------------+---------------------+----------------+------------+-------+-----------------+----------------+-------+-----------------+--------------------+\n",
            "|1      | ACADEMY DINOSAUR| A Epic Drama of a Feminist And a Mad Scientist who must Battle a Teacher in The Canadian Rockies                     |2006         |1           | NULL                |6               | 0.99       |86     | 20.99           |76750           | PG    | Deleted Scenes  | 2020-01-25 14:40:46|\n",
            "|2      | ACE GOLDFINGER  | A Astounding Epistle of a Database Administrator And a Explorer who must Find a Car in Ancient China                 |2006         |1           | NULL                |3               | 4.99       |48     | 12.99           |19350           | G     | Trailers        | 2020-01-25 14:40:46|\n",
            "|3      | ADAPTATION HOLES| A Astounding Reflection of a Lumberjack And a Car who must Sink a Lumberjack in A Baloon Factory                     |2006         |1           | NULL                |7               | 2.99       |50     | 18.99           |20700           | NC-17 | Trailers        | 2020-01-25 14:40:46|\n",
            "|4      | AFFAIR PREJUDICE| A Fanciful Documentary of a Frisbee And a Lumberjack who must Chase a Monkey in A Shark Tank                         |x2006        |1           | NULL                |5               | 2.99       |117    | 26.99           |45500           | G     | Commentaries    | 2020-01-25 14:40:46|\n",
            "|5      | AFRICAN EGG     | A Fast-Paced Documentary of a Pastry Chef And a Dentist who must Pursue a Forensic Psychologist in The Gulf of Mexico|2006         |1           | NULL                |6               | 2.99       |130    | 22.99           |11300           | G     | Deleted Scenes  | 2020-01-25 14:40:46|\n",
            "+-------+-----------------+----------------------------------------------------------------------------------------------------------------------+-------------+------------+---------------------+----------------+------------+-------+-----------------+----------------+-------+-----------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "\n",
            "Número total de registros:\n",
            "1003\n",
            "\n",
            "Nombres de columnas:\n",
            "['film_id', ' title', ' description', ' release_year', ' language_id', ' original_language_id', ' rental_duration', ' rental_rate', ' length', ' replacement_cost', ' num_voted_users', ' rating', ' special_features', 'last_update']\n",
            "\n",
            "=== Inspección del DataFrame: df_customer ===\n",
            "Esquema del DataFrame:\n",
            "root\n",
            " |-- customer_id: string (nullable = true)\n",
            " |--  store_id: string (nullable = true)\n",
            " |--  first_name: string (nullable = true)\n",
            " |--  last_name: string (nullable = true)\n",
            " |--  email: string (nullable = true)\n",
            " |--  address_id: string (nullable = true)\n",
            " |--  active: string (nullable = true)\n",
            " |--  create_date: string (nullable = true)\n",
            " |--  last_update: string (nullable = true)\n",
            " |--  customer_id_old: string (nullable = true)\n",
            " |--  segment: string (nullable = true)\n",
            "\n",
            "\n",
            "Primeras 5 filas:\n",
            "+-----------+---------+-----------+----------+------------------------------------+-----------+-------+--------------------+--------------------+----------------+--------+\n",
            "|customer_id| store_id| first_name| last_name| email                              | address_id| active| create_date        | last_update        | customer_id_old| segment|\n",
            "+-----------+---------+-----------+----------+------------------------------------+-----------+-------+--------------------+--------------------+----------------+--------+\n",
            "|1          |1        | MARY      | SMITH    | MARY.SMITH@sakilacustomer.org      |5          |1      | 2006-02-14 22:04:36| 2006-02-15 04:57:20| NULL           | NULL   |\n",
            "|2          |1        | PATRICIA  | JOHNSON  | PATRICIA.JOHNSON@sakilacustomer.org|6          |1      | 2006-02-14 22:04:36| 2006-02-15 04:57:20| NULL           | NULL   |\n",
            "|3          |1        | LINDA     | WILLIAMS | LINDA.WILLIAMS@sakilacustomer.org  |7          |1      | 2006-02-14 22:04:36| 2006-02-15 04:57:20| NULL           | NULL   |\n",
            "|4          |2        | BARBARA   | JONES    | BARBARA.JONES@sakilacustomer.org   |8          |1      | 2006-02-14 22:04:36| 2006-02-15 04:57:20| NULL           | NULL   |\n",
            "|5          |1        | ELIZABETH | BROWN    | ELIZABETH.BROWN@sakilacustomer.org |9          |1      | 2006-02-14 22:04:36| 2006-02-15 04:57:20| NULL           | NULL   |\n",
            "+-----------+---------+-----------+----------+------------------------------------+-----------+-------+--------------------+--------------------+----------------+--------+\n",
            "only showing top 5 rows\n",
            "\n",
            "\n",
            "Número total de registros:\n",
            "1392\n",
            "\n",
            "Nombres de columnas:\n",
            "['customer_id', ' store_id', ' first_name', ' last_name', ' email', ' address_id', ' active', ' create_date', ' last_update', ' customer_id_old', ' segment']\n",
            "\n",
            "=== Inspección del DataFrame: df_inventory ===\n",
            "Esquema del DataFrame:\n",
            "root\n",
            " |-- inventory_id: string (nullable = true)\n",
            " |-- film_id: string (nullable = true)\n",
            " |--  store_id: string (nullable = true)\n",
            " |--  last_update: string (nullable = true)\n",
            "\n",
            "\n",
            "Primeras 5 filas:\n",
            "+------------+-------+---------+--------------------+\n",
            "|inventory_id|film_id| store_id| last_update        |\n",
            "+------------+-------+---------+--------------------+\n",
            "|1           |1      |1        | 2006-02-15 05:09:17|\n",
            "|2           |1      |1        | 2006-02-15 05:09:17|\n",
            "|3           |1      |1        | 2006-02-15 05:09:17|\n",
            "|4           |1      |1        | 2006-02-15 05:09:17|\n",
            "|5           |1      |2        | 2006-02-15 05:09:17|\n",
            "+------------+-------+---------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "\n",
            "Número total de registros:\n",
            "4581\n",
            "\n",
            "Nombres de columnas:\n",
            "['inventory_id', 'film_id', ' store_id', ' last_update']\n",
            "\n",
            "=== Inspección del DataFrame: df_rental ===\n",
            "Esquema del DataFrame:\n",
            "root\n",
            " |-- rental_id: string (nullable = true)\n",
            " |--  rental_date: string (nullable = true)\n",
            " |--  inventory_id: string (nullable = true)\n",
            " |--  customer_id: string (nullable = true)\n",
            " |--  return_date: string (nullable = true)\n",
            " |--  staff_id: string (nullable = true)\n",
            " |--  last_update: string (nullable = true)\n",
            "\n",
            "\n",
            "Primeras 5 filas:\n",
            "+---------+--------------------+-------------+------------+--------------------+---------+--------------------+\n",
            "|rental_id| rental_date        | inventory_id| customer_id| return_date        | staff_id| last_update        |\n",
            "+---------+--------------------+-------------+------------+--------------------+---------+--------------------+\n",
            "|32       | 2005-05-25 04:06:21|3832         |230         | 2005-05-25 23:55:21|1        | 2006-02-15 21:30:53|\n",
            "|21       | 2005-05-25 01:59:46|146          |388         | 2005-05-26 01:01:46|2        | 2006-02-15 21:30:53|\n",
            "|14       | 2005-05-25 00:31:15|2701         |446         | 2005-05-26 02:56:15|1        | 2006-02-15 21:30:53|\n",
            "|16       | 2005-05-25 00:43:11|389          |316         | 2005-05-26 04:42:11|2        | 2006-02-15 21:30:53|\n",
            "|22       | 2005-05-25 02:19:23|727          |509         | 2005-05-26 04:52:23|2        | 2006-02-15 21:30:53|\n",
            "+---------+--------------------+-------------+------------+--------------------+---------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "\n",
            "Número total de registros:\n",
            "16044\n",
            "\n",
            "Nombres de columnas:\n",
            "['rental_id', ' rental_date', ' inventory_id', ' customer_id', ' return_date', ' staff_id', ' last_update']\n",
            "\n",
            "=== Inspección del DataFrame: df_store ===\n",
            "Esquema del DataFrame:\n",
            "root\n",
            " |-- store_id: string (nullable = true)\n",
            " |-- manager_staff_id: string (nullable = true)\n",
            " |--  address_id: string (nullable = true)\n",
            " |--  last_update: string (nullable = true)\n",
            "\n",
            "\n",
            "Primeras 5 filas:\n",
            "+--------+----------------+-----------+--------------------+\n",
            "|store_id|manager_staff_id| address_id| last_update        |\n",
            "+--------+----------------+-----------+--------------------+\n",
            "|1       |1               |1          | 2016-02-15 04:57:12|\n",
            "|2       |2               |2          | 2016-02-15 04:57:12|\n",
            "+--------+----------------+-----------+--------------------+\n",
            "\n",
            "\n",
            "Número total de registros:\n",
            "2\n",
            "\n",
            "Nombres de columnas:\n",
            "['store_id', 'manager_staff_id', ' address_id', ' last_update']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LIMPIEZA DE TABLAS**"
      ],
      "metadata": {
        "id": "Uvj3uNySxyKG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#LIMPIEZA DE ESPACIOS\n",
        "\n",
        "from pyspark.sql.functions import trim, col\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "def limpiar_espacios(df):\n",
        "\n",
        "    df = df.toDF(*[c.strip() for c in df.columns])\n",
        "\n",
        "\n",
        "    for c in df.columns:\n",
        "        if df.schema[c].dataType == StringType():\n",
        "            df = df.withColumn(c, trim(col(c)))\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "EdR761Q18SU7"
      },
      "execution_count": 205,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_film_sinespacios = limpiar_espacios(df_film)\n",
        "df_customer_sinespacios = limpiar_espacios(df_customer)\n",
        "df_inventory_sinespacios= limpiar_espacios(df_inventory)\n",
        "df_rental_sinespacios = limpiar_espacios(df_rental)\n",
        "df_store_sinespacios = limpiar_espacios(df_store)"
      ],
      "metadata": {
        "id": "FCJPe__7_yD4"
      },
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CAMBIO DE VALORES NULL Y VACIOS POR None\n",
        "\n",
        "from pyspark.sql.functions import when, col, trim\n",
        "\n",
        "\n",
        "def reemplazar_nulls(df):\n",
        "    for c in df.columns:\n",
        "        if df.schema[c].dataType == StringType():\n",
        "            df = df.withColumn(\n",
        "                c,\n",
        "                when(\n",
        "                    (trim(col(c)) == \"NULL\") |\n",
        "                    (trim(col(c)) == \"Null\") |\n",
        "                    (trim(col(c)) == \"\"),\n",
        "                    None\n",
        "                ).otherwise(trim(col(c)))\n",
        "            )\n",
        "    return df"
      ],
      "metadata": {
        "id": "4aMc_av0BE0R"
      },
      "execution_count": 207,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_film_None = reemplazar_nulls(df_film_sinespacios)\n",
        "df_customer_None = reemplazar_nulls(df_customer_sinespacios)\n",
        "df_inventory_None = reemplazar_nulls(df_inventory_sinespacios)\n",
        "df_rental_None = reemplazar_nulls(df_rental_sinespacios)\n",
        "df_store_None = reemplazar_nulls(df_store_sinespacios)"
      ],
      "metadata": {
        "id": "QZPgvI2tBdVg"
      },
      "execution_count": 208,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ELIMINAR COLUMNAS CON 100% VALORES NULOS\n",
        "\n",
        "def eliminar_columnas_nulas(df):\n",
        "    non_null_cols = [c for c in df.columns if df.filter(col(c).isNotNull()).count() > 0]\n",
        "    return df.select(*non_null_cols)"
      ],
      "metadata": {
        "id": "lCkpTk_xHTUH"
      },
      "execution_count": 209,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_film_sincolumnasnulas = eliminar_columnas_nulas(df_film_None)\n",
        "df_inventory_sincolumnasnulas = eliminar_columnas_nulas(df_inventory_None)\n",
        "df_rental_sincolumnasnulas = eliminar_columnas_nulas(df_rental_None)\n",
        "df_customer_sincolumnasnulas = eliminar_columnas_nulas(df_customer_None)\n",
        "df_store_sincolumnasnulas = eliminar_columnas_nulas(df_store_None)"
      ],
      "metadata": {
        "id": "pUvGJFA8Hdz5"
      },
      "execution_count": 210,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Debido a que las columnas customer_id_old y segment  cuentan con un 43% de datos nulos,\n",
        "#se decide eliminarlas, ya que no son columnas criticas ni indispensables para el análisis\n",
        "df_customer_sincolumnasnulas = df_customer_sincolumnasnulas.drop('customer_id_old','segment')"
      ],
      "metadata": {
        "id": "Ttga5YhdsZcx"
      },
      "execution_count": 212,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#LIMPIAR COLUMNAS NUMERICAS\n",
        "\n",
        "from pyspark.sql.functions import regexp_replace, col\n",
        "\n",
        "def limpiar_columnas_numericas(df, columnas):\n",
        "    for c in columnas:\n",
        "        df = df.withColumn(c, regexp_replace(col(c), \"[^0-9]\", \"\"))\n",
        "    return df"
      ],
      "metadata": {
        "id": "uuzwIWR9V6Md"
      },
      "execution_count": 213,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columnas_numericas_df_film = ['film_id','release_year','language_id','rental_duration','length','num_voted_users']\n",
        "columnas_numericas_df_inventory = ['inventory_id', 'film_id', 'store_id']\n",
        "columnas_numericas_df_rental = ['rental_id','inventory_id','customer_id','staff_id']\n",
        "columnas_numericas_df_customer = ['customer_id','store_id','address_id','active']\n",
        "columnas_numericas_df_store = ['store_id', 'manager_staff_id', 'address_id']\n",
        "\n",
        "df_film_limpio = limpiar_columnas_numericas(df_film_sincolumnasnulas, columnas_numericas_df_film)\n",
        "df_inventory_limpio = limpiar_columnas_numericas(df_inventory_sincolumnasnulas, columnas_numericas_df_inventory)\n",
        "df_rental_limpio = limpiar_columnas_numericas(df_rental_sincolumnasnulas, columnas_numericas_df_rental)\n",
        "df_customer_limpio = limpiar_columnas_numericas(df_customer_sincolumnasnulas, columnas_numericas_df_customer)\n",
        "df_store_limpio = limpiar_columnas_numericas(df_store_sincolumnasnulas, columnas_numericas_df_store)"
      ],
      "metadata": {
        "id": "BjJmRUNMXCx5"
      },
      "execution_count": 214,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#LIMPIAR COLUMNAS DECIMALES\n",
        "\n",
        "def limpiar_caracteres_columnas_decimales(df, columnas):\n",
        "    for c in columnas:\n",
        "        df = df.withColumn(c, regexp_replace(col(c), r\"[^0-9.]\", \"\"))\n",
        "    return df\n",
        "\n",
        "def limpiar_columnas_decimales(df, columnas):\n",
        "    for c in columnas:\n",
        "        df = df.withColumn(c, regexp_replace(col(c), \",\", \".\"))\n",
        "    return df"
      ],
      "metadata": {
        "id": "Atrdyg62gIDl"
      },
      "execution_count": 215,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columnas_decimales_df_film = ['rental_rate', 'replacement_cost']\n",
        "df_film_limpio = limpiar_caracteres_columnas_decimales(df_film_limpio, columnas_decimales_df_film)\n",
        "df_film_limpio = limpiar_columnas_decimales(df_film_limpio, columnas_decimales_df_film)"
      ],
      "metadata": {
        "id": "K-YOKL6XgJw6"
      },
      "execution_count": 216,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RENOMBRAR COLUMNAS**"
      ],
      "metadata": {
        "id": "V7aLKr93x-Yq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#RENOMBRAR COLUMNAS\n",
        "\n",
        "df_film_limpio = df_film_limpio.withColumnRenamed(\"last_update\", \"last_update_film\")\n",
        "df_inventory_limpio = df_inventory_limpio.withColumnRenamed(\"last_update\", \"last_update_inventory\")\n",
        "df_rental_limpio = df_rental_limpio.withColumnRenamed(\"last_update\", \"last_update_rental\")\n",
        "df_customer_limpio = df_customer_limpio.withColumnRenamed(\"last_update\", \"last_update_customer\")\n",
        "df_store_limpio = df_store_limpio.withColumnRenamed(\"last_update\", \"last_update_store\")\n",
        "\n",
        "df_store_limpio = df_store_limpio.withColumnRenamed(\"address_id\", \"address_id_store\")\n",
        "df_customer_limpio = df_customer_limpio.withColumnRenamed(\"address_id\", \"address_id_customer\")"
      ],
      "metadata": {
        "id": "tmXqBVHBrQu-"
      },
      "execution_count": 217,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CAMBIO DE TIPO DE DATOS**"
      ],
      "metadata": {
        "id": "3-h__3-yyAQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "df_film_final = (\n",
        "    df_film_limpio\n",
        "    .withColumn(\"film_id\", col(\"film_id\").cast(ShortType()))\n",
        "    .withColumn(\"title\", col(\"title\").cast(StringType()))\n",
        "    .withColumn(\"description\", col(\"description\").cast(StringType()))\n",
        "    .withColumn(\"release_year\", col(\"release_year\").cast(IntegerType()))\n",
        "    .withColumn(\"language_id\", col(\"language_id\").cast(ByteType()))\n",
        "    .withColumn(\"rental_duration\", col(\"rental_duration\").cast(ByteType()))\n",
        "    .withColumn(\"rental_rate\", col(\"rental_rate\").cast(DecimalType(4, 2)))\n",
        "    .withColumn(\"length\", col(\"length\").cast(ShortType()))\n",
        "    .withColumn(\"replacement_cost\", col(\"replacement_cost\").cast(DecimalType(5, 2)))\n",
        "    .withColumn(\"num_voted_users\", col(\"num_voted_users\").cast(IntegerType()))\n",
        "    .withColumn(\"rating\", col(\"rating\").cast(StringType()))\n",
        "    .withColumn(\"special_features\", col(\"special_features\").cast(StringType()))\n",
        "    .withColumn(\"last_update_film\", col(\"last_update_film\").cast(TimestampType()))\n",
        "\n",
        ")\n",
        "\n",
        "df_inventory_final = (\n",
        "    df_inventory_limpio\n",
        "    .withColumn(\"inventory_id\", col(\"inventory_id\").cast(IntegerType()))\n",
        "    .withColumn(\"film_id\", col(\"film_id\").cast(IntegerType()))\n",
        "    .withColumn(\"store_id\", col(\"store_id\").cast(IntegerType()))\n",
        "    .withColumn(\"last_update_inventory\", col(\"last_update_inventory\").cast(TimestampType()))\n",
        ")\n",
        "\n",
        "df_rental_final = (\n",
        "    df_rental_limpio\n",
        "    .withColumn(\"rental_id\", col(\"rental_id\").cast(IntegerType()))\n",
        "    .withColumn(\"rental_date\", col(\"rental_date\").cast(TimestampType()))\n",
        "    .withColumn(\"inventory_id\", col(\"inventory_id\").cast(IntegerType()))\n",
        "    .withColumn(\"customer_id\", col(\"customer_id\").cast(ShortType()))\n",
        "    .withColumn(\"return_date\", col(\"return_date\").cast(TimestampType()))\n",
        "    .withColumn(\"staff_id\", col(\"staff_id\").cast(ByteType()))\n",
        "    .withColumn(\"last_update_rental\", col(\"last_update_rental\").cast(TimestampType()))\n",
        ")\n",
        "\n",
        "df_customer_final = (\n",
        "    df_customer_limpio\n",
        "    .withColumn(\"customer_id\", col(\"customer_id\").cast(ShortType()))\n",
        "    .withColumn(\"store_id\", col(\"store_id\").cast(ByteType()))\n",
        "    .withColumn(\"first_name\", col(\"first_name\").cast(StringType()))\n",
        "    .withColumn(\"last_name\", col(\"last_name\").cast(StringType()))\n",
        "    .withColumn(\"email\", col(\"email\").cast(StringType()))\n",
        "    .withColumn(\"address_id_customer\", col(\"address_id_customer\").cast(ShortType()))\n",
        "    .withColumn(\"active\", col(\"active\").cast(BooleanType()))\n",
        "    .withColumn(\"create_date\", col(\"create_date\").cast(DateType()))\n",
        "    .withColumn(\"last_update_customer\", col(\"last_update_customer\").cast(TimestampType()))\n",
        ")\n",
        "\n",
        "df_store_final = (\n",
        "    df_store_limpio\n",
        "    .withColumn(\"store_id\", col(\"store_id\").cast(ByteType()))\n",
        "    .withColumn(\"manager_staff_id\", col(\"manager_staff_id\").cast(ByteType()))\n",
        "    .withColumn(\"address_id_store\", col(\"address_id_store\").cast(ShortType()))\n",
        "    .withColumn(\"last_update_store\", col(\"last_update_store\").cast(TimestampType()))\n",
        ")"
      ],
      "metadata": {
        "id": "BXZil_QckOSg"
      },
      "execution_count": 248,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#VERIFICAR Y ELIMINAR FILAS DUPLICADAS EN CADA TABLA\n",
        "\n",
        "def eliminar_duplicados(df, nombre_tabla):\n",
        "    print(f\"Filas antes de eliminar duplicados en {nombre_tabla}: {df.count()}\")\n",
        "    df_sin_duplicados = df.dropDuplicates()\n",
        "    print(f\"Filas después de eliminar duplicados en {nombre_tabla}: {df_sin_duplicados.count()}\")\n",
        "    return df_sin_duplicados\n",
        "\n",
        "df_film_final = eliminar_duplicados(df_film_final, \"df_film_final\")\n",
        "df_inventory_final = eliminar_duplicados(df_inventory_final, \"df_inventory_final\")\n",
        "df_rental_final = eliminar_duplicados(df_rental_final, \"df_rental_final\")\n",
        "df_customer_final = eliminar_duplicados(df_customer_final, \"df_customer_final\")\n",
        "df_store_final = eliminar_duplicados(df_store_final, \"df_store_final\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6ADQGmWoc0m",
        "outputId": "5b3be9ea-f3ce-4080-b54d-a6567d63bd50"
      },
      "execution_count": 249,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filas antes de eliminar duplicados en df_film_final: 1003\n",
            "Filas después de eliminar duplicados en df_film_final: 1000\n",
            "Filas antes de eliminar duplicados en df_inventory_final: 4581\n",
            "Filas después de eliminar duplicados en df_inventory_final: 4581\n",
            "Filas antes de eliminar duplicados en df_rental_final: 16044\n",
            "Filas después de eliminar duplicados en df_rental_final: 16044\n",
            "Filas antes de eliminar duplicados en df_customer_final: 1392\n",
            "Filas después de eliminar duplicados en df_customer_final: 1392\n",
            "Filas antes de eliminar duplicados en df_store_final: 2\n",
            "Filas después de eliminar duplicados en df_store_final: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Se crea una columna de Return_Status basada en diferentes condicionales y dependencias de otras columnas,\n",
        "#para no eliminar filas con valores nulos que son importantes para el analisis.\n",
        "\n",
        "from pyspark.sql.functions import col, when, datediff, avg, lit\n",
        "\n",
        "\n",
        "avg_days = df_rental_final.filter(col(\"return_date\").isNotNull()).select(avg(datediff(col(\"return_date\"), col(\"rental_date\")))).first()[0]\n",
        "\n",
        "\n",
        "df_rental_final = df_rental_final.withColumn(\n",
        "    \"return_status\",\n",
        "    when(\n",
        "        col(\"return_date\").isNotNull() & col(\"rental_date\").isNotNull(),\n",
        "        \"returned\"\n",
        "    ).when(\n",
        "        col(\"return_date\").isNull() &\n",
        "        (datediff(col(\"last_update_rental\"), col(\"rental_date\")) > lit(avg_days * 3)),\n",
        "        \"lost\"\n",
        "    ).when(\n",
        "        col(\"return_date\").isNull() &\n",
        "        (datediff(col(\"last_update_rental\"), col(\"rental_date\")) <= lit(avg_days * 3)),\n",
        "        \"rented\"\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "ObhHrMSF5s-w"
      },
      "execution_count": 250,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#lIMPIEZA DE NULLS EN TABLA CUSTOMER\n",
        "df_customer_final = df_customer_final.fillna({\"last_name\": \"no last name\"})"
      ],
      "metadata": {
        "id": "JuY7TGJID6Ot"
      },
      "execution_count": 251,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CONVERTIR TODOS LOS STRINGS EN MAYUSCULA\n",
        "from pyspark.sql.functions import upper\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "def to_uppercase_strings(df):\n",
        "    for column, dtype in df.dtypes:\n",
        "        if dtype == \"string\":\n",
        "            df = df.withColumn(column, upper(df[column]))\n",
        "    return df\n",
        "\n",
        "df_film_final = to_uppercase_strings(df_film_final)\n",
        "df_rental_final = to_uppercase_strings(df_rental_final)\n",
        "df_customer_final = to_uppercase_strings(df_customer_final)"
      ],
      "metadata": {
        "id": "SxftrIGBFknk"
      },
      "execution_count": 252,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PORCENTAJE DE VALORES NULOS POR COLUMNA**"
      ],
      "metadata": {
        "id": "5M6SL-qQMgo_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#SE REVISA EL % DE VALORES NULOS EN CADA COLUMNA, CON EL FIN DE TOMAR DE DECISIONES SOBRE ESTAS\n",
        "\n",
        "from pyspark.sql.functions import col, when, count\n",
        "\n",
        "def porcentaje_nulos_por_tabla(tablas_dict):\n",
        "    resultados = {}\n",
        "    for nombre_tabla, df in tablas_dict.items():\n",
        "        total_filas = df.count()\n",
        "        porcentaje_nulos = df.select([\n",
        "            ((count(when(col(c).isNull(), c)) / total_filas) * 100).alias(c) for c in df.columns\n",
        "        ])\n",
        "        resultados[nombre_tabla] = porcentaje_nulos\n",
        "    return resultados\n",
        "\n",
        "tablas = {\n",
        "    \"film\": df_film_final,\n",
        "    \"inventory\": df_inventory_final,\n",
        "    \"rental\": df_rental_final,\n",
        "    \"customer\": df_customer_final,\n",
        "    \"store\": df_store_final\n",
        "}\n",
        "\n",
        "porcentajes_nulos = porcentaje_nulos_por_tabla(tablas)\n",
        "\n",
        "# Mostrar los resultados\n",
        "for nombre_tabla, df_porcentaje in porcentajes_nulos.items():\n",
        "    print(f\"Porcentaje de nulos en la tabla '{nombre_tabla}':\")\n",
        "    df_porcentaje.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fiaCB_MMfQs",
        "outputId": "086b2916-61ef-4486-d35b-7433d38f171d"
      },
      "execution_count": 283,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Porcentaje de nulos en la tabla 'film':\n",
            "+-------+-----+-----------+------------+-----------+---------------+-----------+------+----------------+---------------+------+----------------+----------------+\n",
            "|film_id|title|description|release_year|language_id|rental_duration|rental_rate|length|replacement_cost|num_voted_users|rating|special_features|last_update_film|\n",
            "+-------+-----+-----------+------------+-----------+---------------+-----------+------+----------------+---------------+------+----------------+----------------+\n",
            "|0.0    |0.0  |0.0        |0.0         |0.0        |0.0            |0.0        |0.0   |0.0             |0.0            |0.0   |0.0             |0.0             |\n",
            "+-------+-----+-----------+------------+-----------+---------------+-----------+------+----------------+---------------+------+----------------+----------------+\n",
            "\n",
            "Porcentaje de nulos en la tabla 'inventory':\n",
            "+------------+-------+--------+---------------------+\n",
            "|inventory_id|film_id|store_id|last_update_inventory|\n",
            "+------------+-------+--------+---------------------+\n",
            "|0.0         |0.0    |0.0     |0.0                  |\n",
            "+------------+-------+--------+---------------------+\n",
            "\n",
            "Porcentaje de nulos en la tabla 'rental':\n",
            "+---------+-----------+------------+-----------+------------------+--------+------------------+-------------+\n",
            "|rental_id|rental_date|inventory_id|customer_id|return_date       |staff_id|last_update_rental|return_status|\n",
            "+---------+-----------+------------+-----------+------------------+--------+------------------+-------------+\n",
            "|0.0      |0.0        |0.0         |0.0        |1.1406133133881824|0.0     |0.0               |0.0          |\n",
            "+---------+-----------+------------+-----------+------------------+--------+------------------+-------------+\n",
            "\n",
            "Porcentaje de nulos en la tabla 'customer':\n",
            "+-----------+--------+----------+---------+-----+-------------------+------+-----------+--------------------+\n",
            "|customer_id|store_id|first_name|last_name|email|address_id_customer|active|create_date|last_update_customer|\n",
            "+-----------+--------+----------+---------+-----+-------------------+------+-----------+--------------------+\n",
            "|0.0        |0.0     |0.0       |0.0      |0.0  |0.0                |0.0   |0.0        |0.0                 |\n",
            "+-----------+--------+----------+---------+-----+-------------------+------+-----------+--------------------+\n",
            "\n",
            "Porcentaje de nulos en la tabla 'store':\n",
            "+--------+----------------+----------------+-----------------+\n",
            "|store_id|manager_staff_id|address_id_store|last_update_store|\n",
            "+--------+----------------+----------------+-----------------+\n",
            "|0.0     |0.0             |0.0             |0.0              |\n",
            "+--------+----------------+----------------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Modelado entidad-relación**"
      ],
      "metadata": {
        "id": "qlliO6x5yIxk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#MER\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "def generar_df_analitico(df_rental_final, df_customer_final, df_store_final, df_inventory_final, df_film_final):\n",
        "    # Renombramos columnas clave para evitar ambigüedades\n",
        "    df_customer_final = df_customer_final.withColumnRenamed(\"store_id\", \"customer_store_id\")\n",
        "    df_store_final = df_store_final.withColumnRenamed(\"store_id\", \"store_id_store\")\n",
        "    df_inventory_final = df_inventory_final.withColumnRenamed(\"store_id\", \"store_id_inventory\")\n",
        "\n",
        "    return (\n",
        "        df_rental_final\n",
        "        .join(df_customer_final, on=\"customer_id\", how=\"inner\")\n",
        "        .join(df_store_final, col(\"customer_store_id\") == col(\"store_id_store\"), how=\"inner\")\n",
        "        .join(df_inventory_final, on=\"inventory_id\", how=\"inner\")\n",
        "        .join(df_film_final, on=\"film_id\", how=\"inner\")\n",
        "    )\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "2jo9d0OwVPA-"
      },
      "execution_count": 292,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_analitico_final = generar_df_analitico(\n",
        "    df_rental_final,\n",
        "    df_customer_final,\n",
        "    df_store_final,\n",
        "    df_inventory_final,\n",
        "    df_film_final\n",
        ")"
      ],
      "metadata": {
        "id": "GY0PHXpaWgab"
      },
      "execution_count": 293,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_analitico_final.printSchema()\n",
        "df_analitico_final.show(5, truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jPCfFqAW1QW",
        "outputId": "492976a8-70bd-4989-d42f-63d1201d9fe5"
      },
      "execution_count": 294,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- film_id: integer (nullable = true)\n",
            " |-- inventory_id: integer (nullable = true)\n",
            " |-- customer_id: short (nullable = true)\n",
            " |-- rental_id: integer (nullable = true)\n",
            " |-- rental_date: timestamp (nullable = true)\n",
            " |-- return_date: timestamp (nullable = true)\n",
            " |-- staff_id: byte (nullable = true)\n",
            " |-- last_update_rental: timestamp (nullable = true)\n",
            " |-- return_status: string (nullable = true)\n",
            " |-- customer_store_id: byte (nullable = true)\n",
            " |-- first_name: string (nullable = true)\n",
            " |-- last_name: string (nullable = false)\n",
            " |-- email: string (nullable = true)\n",
            " |-- address_id_customer: short (nullable = true)\n",
            " |-- active: boolean (nullable = true)\n",
            " |-- create_date: date (nullable = true)\n",
            " |-- last_update_customer: timestamp (nullable = true)\n",
            " |-- store_id_store: byte (nullable = true)\n",
            " |-- manager_staff_id: byte (nullable = true)\n",
            " |-- address_id_store: short (nullable = true)\n",
            " |-- last_update_store: timestamp (nullable = true)\n",
            " |-- store_id_inventory: integer (nullable = true)\n",
            " |-- last_update_inventory: timestamp (nullable = true)\n",
            " |-- title: string (nullable = true)\n",
            " |-- description: string (nullable = true)\n",
            " |-- release_year: integer (nullable = true)\n",
            " |-- language_id: byte (nullable = true)\n",
            " |-- rental_duration: byte (nullable = true)\n",
            " |-- rental_rate: decimal(4,2) (nullable = true)\n",
            " |-- length: short (nullable = true)\n",
            " |-- replacement_cost: decimal(5,2) (nullable = true)\n",
            " |-- num_voted_users: integer (nullable = true)\n",
            " |-- rating: string (nullable = true)\n",
            " |-- special_features: string (nullable = true)\n",
            " |-- last_update_film: timestamp (nullable = true)\n",
            "\n",
            "+-------+------------+-----------+---------+-------------------+-------------------+--------+-------------------+-------------+-----------------+----------+---------+---------------------------------+-------------------+------+-----------+--------------------+--------------+----------------+----------------+-------------------+------------------+---------------------+---------------+--------------------------------------------------------------------------------------------------------+------------+-----------+---------------+-----------+------+----------------+---------------+------+----------------+-------------------+\n",
            "|film_id|inventory_id|customer_id|rental_id|rental_date        |return_date        |staff_id|last_update_rental |return_status|customer_store_id|first_name|last_name|email                            |address_id_customer|active|create_date|last_update_customer|store_id_store|manager_staff_id|address_id_store|last_update_store  |store_id_inventory|last_update_inventory|title          |description                                                                                             |release_year|language_id|rental_duration|rental_rate|length|replacement_cost|num_voted_users|rating|special_features|last_update_film   |\n",
            "+-------+------------+-----------+---------+-------------------+-------------------+--------+-------------------+-------------+-----------------+----------+---------+---------------------------------+-------------------+------+-----------+--------------------+--------------+----------------+----------------+-------------------+------------------+---------------------+---------------+--------------------------------------------------------------------------------------------------------+------------+-----------+---------------+-----------+------+----------------+---------------+------+----------------+-------------------+\n",
            "|646    |2945        |256        |232      |2005-05-26 11:38:05|2005-05-27 08:42:05|2       |2006-02-15 21:30:53|RETURNED     |2                |MABEL     |HOLLAND  |MABEL.HOLLAND@SAKILACUSTOMER.ORG |261                |true  |2006-02-14 |2006-02-15 04:57:20 |2             |2               |2               |2016-02-15 04:57:12|2                 |2006-02-15 05:09:17  |OUTBREAK DIVINE|A UNBELIEVEABLE YARN OF A DATABASE ADMINISTRATOR AND A WOMAN WHO MUST SUCCUMB A A SHARK IN A U-BOAT     |2006        |1          |6              |0.99       |169   |12.99           |22400          |NC-17 |TRAILERS        |2020-01-25 14:40:46|\n",
            "|300    |1359        |51         |119      |2005-05-25 19:37:02|2005-05-29 23:51:02|2       |2006-02-15 21:30:53|RETURNED     |1                |ALICE     |STEWART  |ALICE.STEWART@SAKILACUSTOMER.ORG |55                 |true  |2006-02-14 |2006-02-15 04:57:20 |1             |1               |1               |2016-02-15 04:57:12|2                 |2006-02-15 05:09:17  |FALCON VOLUME  |A FATEFUL SAGA OF A SUMO WRESTLER AND A HUNTER WHO MUST REDEEM A A SHARK IN NEW ORLEANS                 |2006        |1          |5              |4.99       |102   |21.99           |8250           |PG-13 |COMMENTARIES    |2020-01-25 14:40:46|\n",
            "|773    |3534        |502        |258      |2005-05-26 15:28:14|2005-05-30 18:38:14|2       |2006-02-15 21:30:53|RETURNED     |1                |BRETT     |CORNWELL |BRETT.CORNWELL@SAKILACUSTOMER.ORG|507                |true  |2006-02-14 |2006-02-15 04:57:20 |1             |1               |1               |2016-02-15 04:57:12|2                 |2006-02-15 05:09:17  |SEABISCUIT PUNK|A INSIGHTFUL SAGA OF A MAN AND A FORENSIC PSYCHOLOGIST WHO MUST DISCOVER A MAD COW IN A MYSQL CONVENTION|2006        |1          |6              |2.99       |112   |28.99           |2150           |NC-17 |COMMENTARIES    |2020-01-25 14:40:46|\n",
            "|543    |2479        |124        |1039     |2005-05-31 05:32:29|2005-06-01 06:04:29|2       |2006-02-15 21:30:53|RETURNED     |1                |SHEILA    |WELLS    |SHEILA.WELLS@SAKILACUSTOMER.ORG  |128                |false |2006-02-14 |2006-02-15 04:57:20 |1             |1               |1               |2016-02-15 04:57:12|2                 |2006-02-15 05:09:17  |MADIGAN DORADO |A ASTOUNDING CHARACTER STUDY OF A A SHARK AND A A SHARK WHO MUST DISCOVER A CROCODILE IN THE OUTBACK    |2006        |1          |5              |4.99       |116   |20.99           |28650          |R     |DELETED SCENES  |2020-01-25 14:40:46|\n",
            "|433    |1986        |24         |1077     |2005-05-31 10:22:54|2005-06-02 12:21:54|1       |2006-02-15 21:30:53|RETURNED     |2                |KIMBERLY  |LEE      |KIMBERLY.LEE@SAKILACUSTOMER.ORG  |28                 |true  |2006-02-14 |2006-02-15 04:57:20 |2             |2               |2               |2016-02-15 04:57:12|2                 |2006-02-15 05:09:17  |HORN WORKING   |A STUNNING DISPLAY OF A MAD SCIENTIST AND A TECHNICAL WRITER WHO MUST SUCCUMB A MONKEY IN A SHARK TANK  |2006        |1          |4              |2.99       |95    |23.99           |14850          |PG    |TRAILERS        |2020-01-25 14:40:46|\n",
            "+-------+------------+-----------+---------+-------------------+-------------------+--------+-------------------+-------------+-----------------+----------+---------+---------------------------------+-------------------+------+-----------+--------------------+--------------+----------------+----------------+-------------------+------------------+---------------------+---------------+--------------------------------------------------------------------------------------------------------+------------+-----------+---------------+-----------+------+----------------+---------------+------+----------------+-------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **EDA Análisis exploratorio de datos**"
      ],
      "metadata": {
        "id": "_PZnUAQ51Olb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analisis univariado"
      ],
      "metadata": {
        "id": "DymTVR2k1U6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#VARIABLES NUMERICAS\n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.types import NumericType\n",
        "\n",
        "def analisis_univariado_numerico(df):\n",
        "\n",
        "    columnas_numericas = [f.name for f in df.schema.fields if isinstance(f.dataType, NumericType)]\n",
        "\n",
        "\n",
        "    resumen = df.select(*columnas_numericas).describe()\n",
        "\n",
        "\n",
        "    resumen.show(truncate=False)\n"
      ],
      "metadata": {
        "id": "4qXRmo9U1WMN"
      },
      "execution_count": 295,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analisis_univariado_numerico(df_analitico_final)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LrP_qEJ77pI1",
        "outputId": "ac98c3a0-f7fb-4e80-dff1-7e9ee2764996"
      },
      "execution_count": 296,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------------+------------------+------------------+-----------------+------------------+-------------------+-------------------+-------------------+-------------------+-------------------+--------------------+------------+-----------+-----------------+-----------------+------------------+-----------------+------------------+\n",
            "|summary|film_id           |inventory_id      |customer_id       |rental_id        |staff_id          |customer_store_id  |address_id_customer|store_id_store     |manager_staff_id   |address_id_store   |store_id_inventory  |release_year|language_id|rental_duration  |rental_rate      |length            |replacement_cost |num_voted_users   |\n",
            "+-------+------------------+------------------+------------------+-----------------+------------------+-------------------+-------------------+-------------------+-------------------+-------------------+--------------------+------------+-----------+-----------------+-----------------+------------------+-----------------+------------------+\n",
            "|count  |16044             |16044             |16044             |16044            |16044             |16044              |16044              |16044              |16044              |16044              |16044               |16044       |16044      |16044            |16044            |16044             |16044            |16044             |\n",
            "|mean   |501.10888805784094|2291.8425579655946|297.14316878583895|8025.371478434306|1.49887808526552  |1.4548117676389927 |301.8585140862628  |1.4548117676389927 |1.4548117676389927 |1.4548117676389927 |1.9992520568436798  |2006.0      |1.0        |4.935489902767389|2.942630         |114.97107953128895|20.215462        |39301.98828222389 |\n",
            "|stddev |288.51352853443257|1322.2106432491476|172.45313648154487|4632.777248876796|0.5000143241440549|0.49796935597986214|173.08537080130517 |0.49796935597986214|0.49796935597986214|0.49796935597986214|0.027339171963283424|0.0         |0.0        |1.40168979439409 |1.649677567954177|40.102347231803414|6.081773871681717|22485.385207691626|\n",
            "|min    |1                 |1                 |1                 |1                |1                 |1                  |5                  |1                  |1                  |1                  |1                   |2006        |1          |3                |0.99             |46                |9.99             |0                 |\n",
            "|max    |1000              |4581              |599               |16049            |2                 |2                  |605                |2                  |2                  |2                  |2                   |2006        |1          |7                |4.99             |185               |29.99            |76900             |\n",
            "+-------+------------------+------------------+------------------+-----------------+------------------+-------------------+-------------------+-------------------+-------------------+-------------------+--------------------+------------+-----------+-----------------+-----------------+------------------+-----------------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, desc\n",
        "from pyspark.sql import Row\n",
        "\n",
        "def resumen_categoricas(df, top_n=10):\n",
        "    columnas_categoricas = [f.name for f in df.schema.fields\n",
        "                            if f.dataType.simpleString() in ['string', 'boolean']]\n",
        "\n",
        "    columnas_excluir = ['first_name', 'last_name', 'email', 'description']\n",
        "\n",
        "    columnas_filtradas = [c for c in columnas_categoricas if c not in columnas_excluir]\n",
        "\n",
        "    resumen = []\n",
        "\n",
        "    for columna in columnas_filtradas:\n",
        "        count = df.filter(col(columna).isNotNull()).count()\n",
        "        unique = df.select(columna).distinct().count()\n",
        "        fila_top = df.groupBy(columna).count().orderBy(desc(\"count\")).first()\n",
        "\n",
        "        top = fila_top[columna] if fila_top else None\n",
        "        freq = fila_top[\"count\"] if fila_top else None\n",
        "\n",
        "        resumen.append(Row(\n",
        "            columna=columna,\n",
        "            count=count,\n",
        "            unique=unique,\n",
        "            top=top,\n",
        "            freq=freq\n",
        "        ))\n",
        "\n",
        "    return spark.createDataFrame(resumen)\n"
      ],
      "metadata": {
        "id": "dHTBhoXI724w"
      },
      "execution_count": 297,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resumen_categoricas(df_analitico_final).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2sJHz2WH8Jfm",
        "outputId": "1802a9d0-d062-4934-81b5-ab089f085a8d"
      },
      "execution_count": 298,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+-----+------+------------------+-----+\n",
            "|         columna|count|unique|               top| freq|\n",
            "+----------------+-----+------+------------------+-----+\n",
            "|   return_status|16044|     3|          RETURNED|15861|\n",
            "|          active|16044|     2|              true|15640|\n",
            "|           title|16044|   958|BUCKET BROTHERHOOD|   34|\n",
            "|          rating|16044|     5|             PG-13| 3585|\n",
            "|special_features|16044|     4|          TRAILERS| 8518|\n",
            "+----------------+-----+------+------------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.types import NumericType\n",
        "from pyspark.sql import Row\n",
        "\n",
        "def detectar_outliers_iqr_df(df, precision=0.01):\n",
        "    columnas_numericas = [f.name for f in df.schema.fields\n",
        "                          if isinstance(f.dataType, NumericType) and \"id\" not in f.name.lower()]\n",
        "\n",
        "    resumen = []\n",
        "\n",
        "    for columna in columnas_numericas:\n",
        "        try:\n",
        "            q1, q2, q3 = df.approxQuantile(columna, [0.25, 0.5, 0.75], precision)\n",
        "            iqr = q3 - q1\n",
        "            lim_inf = q1 - 1.5 * iqr\n",
        "            lim_sup = q3 + 1.5 * iqr\n",
        "            count_outliers = df.filter((col(columna) < lim_inf) | (col(columna) > lim_sup)).count()\n",
        "\n",
        "            resumen.append(Row(\n",
        "                columna=columna,\n",
        "                Q1=round(q1, 2),\n",
        "                Q2=round(q2, 2),\n",
        "                Q3=round(q3, 2),\n",
        "                Limite_Inferior=round(lim_inf, 2),\n",
        "                Limite_Superior=round(lim_sup, 2),\n",
        "                Outliers=count_outliers\n",
        "            ))\n",
        "        except:\n",
        "            resumen.append(Row(\n",
        "                columna=columna,\n",
        "                Q1=None,\n",
        "                Q2=None,\n",
        "                Q3=None,\n",
        "                Limite_Inferior=None,\n",
        "                Limite_Superior=None,\n",
        "                Outliers=None\n",
        "            ))\n",
        "\n",
        "    df_resumen = spark.createDataFrame(resumen)\n",
        "    df_resumen.select(\"columna\", \"Q1\", \"Q2\", \"Q3\", \"Limite_Inferior\", \"Limite_Superior\", \"Outliers\") \\\n",
        "              .orderBy(\"columna\") \\\n",
        "              .show(truncate=False)\n",
        "\n",
        "    return df_resumen"
      ],
      "metadata": {
        "id": "NuihI3kRLI5s"
      },
      "execution_count": 299,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "detectar_outliers_iqr_df(df_analitico_final)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUm3EXkmLOAQ",
        "outputId": "dcc7da5b-69d8-4f5f-b398-52fd42326c8d"
      },
      "execution_count": 300,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+-------+-------+-------+---------------+---------------+--------+\n",
            "|columna         |Q1     |Q2     |Q3     |Limite_Inferior|Limite_Superior|Outliers|\n",
            "+----------------+-------+-------+-------+---------------+---------------+--------+\n",
            "|length          |80.0   |112.0  |148.0  |-22.0          |250.0          |0       |\n",
            "|num_voted_users |18300.0|38950.0|58300.0|-41700.0       |118300.0       |0       |\n",
            "|release_year    |2006.0 |2006.0 |2006.0 |2006.0         |2006.0         |0       |\n",
            "|rental_duration |4.0    |5.0    |6.0    |1.0            |9.0            |0       |\n",
            "|rental_rate     |0.99   |2.99   |4.99   |-5.01          |10.99          |0       |\n",
            "|replacement_cost|14.99  |19.99  |24.99  |-0.01          |39.99          |0       |\n",
            "+----------------+-------+-------+-------+---------------+---------------+--------+\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[columna: string, Q1: double, Q2: double, Q3: double, Limite_Inferior: double, Limite_Superior: double, Outliers: bigint]"
            ]
          },
          "metadata": {},
          "execution_count": 300
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, count, avg, desc, isnull, datediff, to_date\n",
        "\n",
        "# 1. Número total de películas, clientes, tiendas, alquileres\n",
        "print(\"Número total de películas, clientes, tiendas y alquileres:\")\n",
        "print(\"Películas:\", df_analitico_final.select(\"film_id\").distinct().count())\n",
        "print(\"Clientes:\", df_analitico_final.select(\"customer_id\").distinct().count())\n",
        "print(\"Tiendas:\", df_analitico_final.select(\"store_id_store\").distinct().count())\n",
        "print(\"Alquileres:\", df_analitico_final.select(\"rental_id\").distinct().count())\n",
        "\n",
        "# 2. ¿Cuántos clientes están activos vs. inactivos?\n",
        "print(\"\\nCantidad de clientes activos vs. inactivos:\")\n",
        "df_analitico_final.groupBy(\"active\").count().show()\n",
        "\n",
        "# 3. Número de películas distintas por tienda\n",
        "print(\"\\nNúmero de películas distintas disponibles por tienda:\")\n",
        "df_analitico_final.select(\"store_id_store\", \"film_id\").distinct().groupBy(\"store_id_store\").count().show()\n",
        "\n",
        "# 4. Top 10 clientes con más alquileres\n",
        "print(\"\\nTop 10 clientes con más alquileres:\")\n",
        "df_analitico_final.groupBy(\"customer_id\").count().orderBy(desc(\"count\")).show(10)\n",
        "\n",
        "# 5. Cantidad de alquileres por tienda\n",
        "print(\"\\nCantidad de alquileres por tienda:\")\n",
        "df_analitico_final.groupBy(\"store_id_store\").count().orderBy(\"store_id_store\").show()\n",
        "\n",
        "# 6. Películas más alquiladas\n",
        "print(\"\\nPelículas con mayor número de alquileres:\")\n",
        "df_analitico_final.groupBy(\"title\").count().orderBy(desc(\"count\")).show(10)\n",
        "\n",
        "# 7. Promedio de alquileres por cliente\n",
        "print(\"\\nPromedio de alquileres por cliente:\")\n",
        "alquileres_por_cliente = df_analitico_final.groupBy(\"customer_id\").count()\n",
        "alquileres_por_cliente.select(avg(\"count\").alias(\"avg_rentals\")).show()\n",
        "\n",
        "# 8. Clientes que no devolvieron películas\n",
        "clientes_lost = df_analitico_final.filter(col(\"return_status\") == \"LOST\").select(\"customer_id\").distinct()\n",
        "print(f\"Total de clientes con películas no devueltas: {clientes_lost.count()}\")\n",
        "clientes_lost.show()\n",
        "\n",
        "# 9. Duración promedio de alquileres (días)\n",
        "print(\"\\nDuración promedio de los alquileres en días:\")\n",
        "df_analitico_final = df_analitico_final.withColumn(\"dias_alquiler\", datediff(\"return_date\", \"rental_date\"))\n",
        "df_analitico_final.select(avg(\"dias_alquiler\").alias(\"promedio_dias\")).show()\n",
        "\n",
        "\n",
        "# 10. Valor de perdidas por peliculas no devueltas\n",
        "from pyspark.sql.functions import col, sum as spark_sum\n",
        "\n",
        "print(\"Ingreso potencial perdido por películas con estado LOST:\")\n",
        "\n",
        "ingreso_perdido_lost = df_analitico_final.filter(col(\"return_status\") == \"LOST\") \\\n",
        "    .agg(spark_sum(\"rental_rate\")) \\\n",
        "    .collect()[0][0]\n",
        "\n",
        "print(f\"Perdidas por peliculas no devueltas: ${ingreso_perdido_lost:.2f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzS_F5kpR-ZC",
        "outputId": "52cfc9e2-4804-43e0-eda4-82337f5730c4"
      },
      "execution_count": 329,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número total de películas, clientes, tiendas y alquileres:\n",
            "Películas: 958\n",
            "Clientes: 599\n",
            "Tiendas: 2\n",
            "Alquileres: 16044\n",
            "\n",
            "Cantidad de clientes activos vs. inactivos:\n",
            "+------+-----+\n",
            "|active|count|\n",
            "+------+-----+\n",
            "|  true|15640|\n",
            "| false|  404|\n",
            "+------+-----+\n",
            "\n",
            "\n",
            "Número de películas distintas disponibles por tienda:\n",
            "+--------------+-----+\n",
            "|store_id_store|count|\n",
            "+--------------+-----+\n",
            "|             1|  957|\n",
            "|             2|  958|\n",
            "+--------------+-----+\n",
            "\n",
            "\n",
            "Top 10 clientes con más alquileres:\n",
            "+-----------+-----+\n",
            "|customer_id|count|\n",
            "+-----------+-----+\n",
            "|        148|   46|\n",
            "|        526|   45|\n",
            "|        236|   42|\n",
            "|        144|   42|\n",
            "|         75|   41|\n",
            "|        197|   40|\n",
            "|        469|   40|\n",
            "|        178|   39|\n",
            "|        137|   39|\n",
            "|        468|   39|\n",
            "+-----------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "\n",
            "Cantidad de alquileres por tienda:\n",
            "+--------------+-----+\n",
            "|store_id_store|count|\n",
            "+--------------+-----+\n",
            "|             1| 8747|\n",
            "|             2| 7297|\n",
            "+--------------+-----+\n",
            "\n",
            "\n",
            "Películas con mayor número de alquileres:\n",
            "+-------------------+-----+\n",
            "|              title|count|\n",
            "+-------------------+-----+\n",
            "| BUCKET BROTHERHOOD|   34|\n",
            "|   ROCKETEER MOTHER|   33|\n",
            "|     GRIT CLOCKWORK|   32|\n",
            "|RIDGEMONT SUBMARINE|   32|\n",
            "|      SCALAWAG DUCK|   32|\n",
            "|     JUGGLER HARDLY|   32|\n",
            "|     FORWARD TEMPLE|   32|\n",
            "|     TIMBERLAND SKY|   31|\n",
            "|  GOODFELLAS SALUTE|   31|\n",
            "|       NETWORK PEAK|   31|\n",
            "+-------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n",
            "\n",
            "Promedio de alquileres por cliente:\n",
            "+------------------+\n",
            "|       avg_rentals|\n",
            "+------------------+\n",
            "|26.784641068447414|\n",
            "+------------------+\n",
            "\n",
            "Total de clientes con películas no devueltas: 1\n",
            "+-----------+\n",
            "|customer_id|\n",
            "+-----------+\n",
            "|        554|\n",
            "+-----------+\n",
            "\n",
            "\n",
            "Duración promedio de los alquileres en días:\n",
            "+-----------------+\n",
            "|    promedio_dias|\n",
            "+-----------------+\n",
            "|5.025219090851775|\n",
            "+-----------------+\n",
            "\n",
            "Ingreso potencial perdido por películas con estado LOST:\n",
            "Perdidas por peliculas no devueltas: $0.99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9_SW6-kgEikU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}