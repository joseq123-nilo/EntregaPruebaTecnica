{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **CODIGO CARGADO EN EL JOB DE AWS GLUE**"
      ],
      "metadata": {
        "id": "6EoEKFxSJaKH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "from pyspark.context import SparkContext\n",
        "from awsglue.context import GlueContext\n",
        "from awsglue.utils import getResolvedOptions\n",
        "from awsglue.job import Job\n",
        "\n",
        "# Definimos los argumentos requeridos por Glue\n",
        "args = getResolvedOptions(sys.argv, ['JOB_NAME'])\n",
        "\n",
        "# Inicializamos Spark y Glue\n",
        "sc = SparkContext()\n",
        "glueContext = GlueContext(sc)\n",
        "spark = glueContext.spark_session\n",
        "\n",
        "# Inicializamos el Job\n",
        "job = Job(glueContext)\n",
        "job.init(args['JOB_NAME'], args)\n",
        "\n",
        "# Cargamos los archivos CSV desde S3\n",
        "df_film = spark.read.option(\"header\", True).option(\"sep\", \";\").csv(\"s3://pruebatecnicajosemqe/Films.csv\")\n",
        "df_customer = spark.read.option(\"header\", True).option(\"sep\", \";\").csv(\"s3://pruebatecnicajosemqe/customer.csv\")\n",
        "df_inventory = spark.read.option(\"header\", True).option(\"sep\", \";\").csv(\"s3://pruebatecnicajosemqe/inventory.csv\")\n",
        "df_rental = spark.read.option(\"header\", True).option(\"sep\", \";\").csv(\"s3://pruebatecnicajosemqe/rental.csv\")\n",
        "df_store = spark.read.option(\"header\", True).option(\"sep\", \";\").csv(\"s3://pruebatecnicajosemqe/store.csv\")\n",
        "\n",
        "#INSPECCION INICIAL DE LAS TABLAS\n",
        "def inspeccion_inicial(df, nombre_df):\n",
        "    print(f\"\\n=== Inspección del DataFrame: {nombre_df} ===\")\n",
        "    print(\"Esquema del DataFrame:\")\n",
        "    df.printSchema()\n",
        "\n",
        "    print(\"\\nPrimeras 5 filas:\")\n",
        "    df.show(5, truncate=False)\n",
        "\n",
        "    print(\"\\nNúmero total de registros:\")\n",
        "    print(df.count())\n",
        "\n",
        "    print(\"\\nNombres de columnas:\")\n",
        "    print(df.columns)\n",
        "\n",
        "inspeccion_inicial(df_film, \"df_film\")\n",
        "inspeccion_inicial(df_customer, \"df_customer\")\n",
        "inspeccion_inicial(df_inventory, \"df_inventory\")\n",
        "inspeccion_inicial(df_rental, \"df_rental\")\n",
        "inspeccion_inicial(df_store, \"df_store\")\n",
        "\n",
        "#LIMPIEZA DE ESPACIOS\n",
        "\n",
        "from pyspark.sql.functions import trim, col\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "def limpiar_espacios(df):\n",
        "\n",
        "    df = df.toDF(*[c.strip() for c in df.columns])\n",
        "\n",
        "\n",
        "    for c in df.columns:\n",
        "        if df.schema[c].dataType == StringType():\n",
        "            df = df.withColumn(c, trim(col(c)))\n",
        "\n",
        "    return df\n",
        "\n",
        "df_film_sinespacios = limpiar_espacios(df_film)\n",
        "df_customer_sinespacios = limpiar_espacios(df_customer)\n",
        "df_inventory_sinespacios= limpiar_espacios(df_inventory)\n",
        "df_rental_sinespacios = limpiar_espacios(df_rental)\n",
        "df_store_sinespacios = limpiar_espacios(df_store)\n",
        "\n",
        "#CAMBIO DE VALORES NULL Y VACIOS POR None\n",
        "\n",
        "from pyspark.sql.functions import when, col, trim\n",
        "\n",
        "\n",
        "def reemplazar_nulls(df):\n",
        "    for c in df.columns:\n",
        "        if df.schema[c].dataType == StringType():\n",
        "            df = df.withColumn(\n",
        "                c,\n",
        "                when(\n",
        "                    (trim(col(c)) == \"NULL\") |\n",
        "                    (trim(col(c)) == \"Null\") |\n",
        "                    (trim(col(c)) == \"\"),\n",
        "                    None\n",
        "                ).otherwise(trim(col(c)))\n",
        "            )\n",
        "    return df\n",
        "\n",
        "df_film_None = reemplazar_nulls(df_film_sinespacios)\n",
        "df_customer_None = reemplazar_nulls(df_customer_sinespacios)\n",
        "df_inventory_None = reemplazar_nulls(df_inventory_sinespacios)\n",
        "df_rental_None = reemplazar_nulls(df_rental_sinespacios)\n",
        "df_store_None = reemplazar_nulls(df_store_sinespacios)\n",
        "\n",
        "#ELIMINAR COLUMNAS CON 100% VALORES NULOS\n",
        "\n",
        "def eliminar_columnas_nulas(df):\n",
        "    non_null_cols = [c for c in df.columns if df.filter(col(c).isNotNull()).count() > 0]\n",
        "    return df.select(*non_null_cols)\n",
        "\n",
        "df_film_sincolumnasnulas = eliminar_columnas_nulas(df_film_None)\n",
        "df_inventory_sincolumnasnulas = eliminar_columnas_nulas(df_inventory_None)\n",
        "df_rental_sincolumnasnulas = eliminar_columnas_nulas(df_rental_None)\n",
        "df_customer_sincolumnasnulas = eliminar_columnas_nulas(df_customer_None)\n",
        "df_store_sincolumnasnulas = eliminar_columnas_nulas(df_store_None)\n",
        "\n",
        "#Debido a que las columnas customer_id_old y segment  cuentan con un 43% de datos nulos,\n",
        "#se decide eliminarlas, ya que no son columnas criticas ni indispensables para el análisis\n",
        "df_customer_sincolumnasnulas = df_customer_sincolumnasnulas.drop('customer_id_old','segment')\n",
        "\n",
        "#LIMPIAR COLUMNAS NUMERICAS\n",
        "\n",
        "from pyspark.sql.functions import regexp_replace, col\n",
        "\n",
        "def limpiar_columnas_numericas(df, columnas):\n",
        "    for c in columnas:\n",
        "        df = df.withColumn(c, regexp_replace(col(c), \"[^0-9]\", \"\"))\n",
        "    return df\n",
        "\n",
        "columnas_numericas_df_film = ['film_id','release_year','language_id','rental_duration','length','num_voted_users']\n",
        "columnas_numericas_df_inventory = ['inventory_id', 'film_id', 'store_id']\n",
        "columnas_numericas_df_rental = ['rental_id','inventory_id','customer_id','staff_id']\n",
        "columnas_numericas_df_customer = ['customer_id','store_id','address_id','active']\n",
        "columnas_numericas_df_store = ['store_id', 'manager_staff_id', 'address_id']\n",
        "\n",
        "df_film_limpio = limpiar_columnas_numericas(df_film_sincolumnasnulas, columnas_numericas_df_film)\n",
        "df_inventory_limpio = limpiar_columnas_numericas(df_inventory_sincolumnasnulas, columnas_numericas_df_inventory)\n",
        "df_rental_limpio = limpiar_columnas_numericas(df_rental_sincolumnasnulas, columnas_numericas_df_rental)\n",
        "df_customer_limpio = limpiar_columnas_numericas(df_customer_sincolumnasnulas, columnas_numericas_df_customer)\n",
        "df_store_limpio = limpiar_columnas_numericas(df_store_sincolumnasnulas, columnas_numericas_df_store)\n",
        "\n",
        "#LIMPIAR COLUMNAS DECIMALES\n",
        "\n",
        "def limpiar_caracteres_columnas_decimales(df, columnas):\n",
        "    for c in columnas:\n",
        "        df = df.withColumn(c, regexp_replace(col(c), r\"[^0-9.]\", \"\"))\n",
        "    return df\n",
        "\n",
        "def limpiar_columnas_decimales(df, columnas):\n",
        "    for c in columnas:\n",
        "        df = df.withColumn(c, regexp_replace(col(c), \",\", \".\"))\n",
        "    return df\n",
        "\n",
        "columnas_decimales_df_film = ['rental_rate', 'replacement_cost']\n",
        "df_film_limpio = limpiar_caracteres_columnas_decimales(df_film_limpio, columnas_decimales_df_film)\n",
        "df_film_limpio = limpiar_columnas_decimales(df_film_limpio, columnas_decimales_df_film)\n",
        "\n",
        "#RENOMBRAR COLUMNAS\n",
        "\n",
        "df_film_limpio = df_film_limpio.withColumnRenamed(\"last_update\", \"last_update_film\")\n",
        "df_inventory_limpio = df_inventory_limpio.withColumnRenamed(\"last_update\", \"last_update_inventory\")\n",
        "df_rental_limpio = df_rental_limpio.withColumnRenamed(\"last_update\", \"last_update_rental\")\n",
        "df_customer_limpio = df_customer_limpio.withColumnRenamed(\"last_update\", \"last_update_customer\")\n",
        "df_store_limpio = df_store_limpio.withColumnRenamed(\"last_update\", \"last_update_store\")\n",
        "\n",
        "df_store_limpio = df_store_limpio.withColumnRenamed(\"address_id\", \"address_id_store\")\n",
        "df_customer_limpio = df_customer_limpio.withColumnRenamed(\"address_id\", \"address_id_customer\")\n",
        "\n",
        "#CAMBIO DE TIPO DE DATOS\n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "df_film_final = (\n",
        "    df_film_limpio\n",
        "    .withColumn(\"film_id\", col(\"film_id\").cast(ShortType()))\n",
        "    .withColumn(\"title\", col(\"title\").cast(StringType()))\n",
        "    .withColumn(\"description\", col(\"description\").cast(StringType()))\n",
        "    .withColumn(\"release_year\", col(\"release_year\").cast(IntegerType()))\n",
        "    .withColumn(\"language_id\", col(\"language_id\").cast(ByteType()))\n",
        "    .withColumn(\"rental_duration\", col(\"rental_duration\").cast(ByteType()))\n",
        "    .withColumn(\"rental_rate\", col(\"rental_rate\").cast(DecimalType(4, 2)))\n",
        "    .withColumn(\"length\", col(\"length\").cast(ShortType()))\n",
        "    .withColumn(\"replacement_cost\", col(\"replacement_cost\").cast(DecimalType(5, 2)))\n",
        "    .withColumn(\"num_voted_users\", col(\"num_voted_users\").cast(IntegerType()))\n",
        "    .withColumn(\"rating\", col(\"rating\").cast(StringType()))\n",
        "    .withColumn(\"special_features\", col(\"special_features\").cast(StringType()))\n",
        "    .withColumn(\"last_update_film\", col(\"last_update_film\").cast(TimestampType()))\n",
        "\n",
        ")\n",
        "\n",
        "df_inventory_final = (\n",
        "    df_inventory_limpio\n",
        "    .withColumn(\"inventory_id\", col(\"inventory_id\").cast(IntegerType()))\n",
        "    .withColumn(\"film_id\", col(\"film_id\").cast(IntegerType()))\n",
        "    .withColumn(\"store_id\", col(\"store_id\").cast(IntegerType()))\n",
        "    .withColumn(\"last_update_inventory\", col(\"last_update_inventory\").cast(TimestampType()))\n",
        ")\n",
        "\n",
        "df_rental_final = (\n",
        "    df_rental_limpio\n",
        "    .withColumn(\"rental_id\", col(\"rental_id\").cast(IntegerType()))\n",
        "    .withColumn(\"rental_date\", col(\"rental_date\").cast(TimestampType()))\n",
        "    .withColumn(\"inventory_id\", col(\"inventory_id\").cast(IntegerType()))\n",
        "    .withColumn(\"customer_id\", col(\"customer_id\").cast(ShortType()))\n",
        "    .withColumn(\"return_date\", col(\"return_date\").cast(TimestampType()))\n",
        "    .withColumn(\"staff_id\", col(\"staff_id\").cast(ByteType()))\n",
        "    .withColumn(\"last_update_rental\", col(\"last_update_rental\").cast(TimestampType()))\n",
        ")\n",
        "\n",
        "df_customer_final = (\n",
        "    df_customer_limpio\n",
        "    .withColumn(\"customer_id\", col(\"customer_id\").cast(ShortType()))\n",
        "    .withColumn(\"store_id\", col(\"store_id\").cast(ByteType()))\n",
        "    .withColumn(\"first_name\", col(\"first_name\").cast(StringType()))\n",
        "    .withColumn(\"last_name\", col(\"last_name\").cast(StringType()))\n",
        "    .withColumn(\"email\", col(\"email\").cast(StringType()))\n",
        "    .withColumn(\"address_id_customer\", col(\"address_id_customer\").cast(ShortType()))\n",
        "    .withColumn(\"active\", col(\"active\").cast(BooleanType()))\n",
        "    .withColumn(\"create_date\", col(\"create_date\").cast(DateType()))\n",
        "    .withColumn(\"last_update_customer\", col(\"last_update_customer\").cast(TimestampType()))\n",
        ")\n",
        "\n",
        "df_store_final = (\n",
        "    df_store_limpio\n",
        "    .withColumn(\"store_id\", col(\"store_id\").cast(ByteType()))\n",
        "    .withColumn(\"manager_staff_id\", col(\"manager_staff_id\").cast(ByteType()))\n",
        "    .withColumn(\"address_id_store\", col(\"address_id_store\").cast(ShortType()))\n",
        "    .withColumn(\"last_update_store\", col(\"last_update_store\").cast(TimestampType()))\n",
        ")\n",
        "\n",
        "#VERIFICAR Y ELIMINAR FILAS DUPLICADAS EN CADA TABLA\n",
        "\n",
        "def eliminar_duplicados(df, nombre_tabla):\n",
        "\n",
        "    df_sin_duplicados = df.dropDuplicates()\n",
        "\n",
        "    return df_sin_duplicados\n",
        "\n",
        "df_film_final = eliminar_duplicados(df_film_final, \"df_film_final\")\n",
        "df_inventory_final = eliminar_duplicados(df_inventory_final, \"df_inventory_final\")\n",
        "df_rental_final = eliminar_duplicados(df_rental_final, \"df_rental_final\")\n",
        "df_customer_final = eliminar_duplicados(df_customer_final, \"df_customer_final\")\n",
        "df_store_final = eliminar_duplicados(df_store_final, \"df_store_final\")\n",
        "\n",
        "#Se crea una columna de Return_Status basada en diferentes condicionales y dependencias de otras columnas,\n",
        "#para no eliminar filas con valores nulos que son importantes para el analisis.\n",
        "\n",
        "from pyspark.sql.functions import col, when, datediff, avg, lit\n",
        "\n",
        "\n",
        "avg_days = df_rental_final.filter(col(\"return_date\").isNotNull()).select(avg(datediff(col(\"return_date\"), col(\"rental_date\")))).first()[0]\n",
        "\n",
        "\n",
        "df_rental_final = df_rental_final.withColumn(\n",
        "    \"return_status\",\n",
        "    when(\n",
        "        col(\"return_date\").isNotNull() & col(\"rental_date\").isNotNull(),\n",
        "        \"returned\"\n",
        "    ).when(\n",
        "        col(\"return_date\").isNull() &\n",
        "        (datediff(col(\"last_update_rental\"), col(\"rental_date\")) > lit(avg_days * 3)),\n",
        "        \"lost\"\n",
        "    ).when(\n",
        "        col(\"return_date\").isNull() &\n",
        "        (datediff(col(\"last_update_rental\"), col(\"rental_date\")) <= lit(avg_days * 3)),\n",
        "        \"rented\"\n",
        "    )\n",
        ")\n",
        "\n",
        "#lIMPIEZA DE NULLS EN TABLA CUSTOMER\n",
        "df_customer_final = df_customer_final.fillna({\"last_name\": \"no last name\"})\n",
        "\n",
        "#CONVERTIR TODOS LOS STRINGS EN MAYUSCULA\n",
        "from pyspark.sql.functions import upper\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "def to_uppercase_strings(df):\n",
        "    for column, dtype in df.dtypes:\n",
        "        if dtype == \"string\":\n",
        "            df = df.withColumn(column, upper(df[column]))\n",
        "    return df\n",
        "\n",
        "df_film_final = to_uppercase_strings(df_film_final)\n",
        "df_rental_final = to_uppercase_strings(df_rental_final)\n",
        "df_customer_final = to_uppercase_strings(df_customer_final)\n",
        "\n",
        "#SE REVISA EL % DE VALORES NULOS EN CADA COLUMNA, CON EL FIN DE TOMAR DE DECISIONES SOBRE ESTAS\n",
        "\n",
        "from pyspark.sql.functions import col, when, count\n",
        "\n",
        "def porcentaje_nulos_por_tabla(tablas_dict):\n",
        "    resultados = {}\n",
        "    for nombre_tabla, df in tablas_dict.items():\n",
        "        total_filas = df.count()\n",
        "        porcentaje_nulos = df.select([\n",
        "            ((count(when(col(c).isNull(), c)) / total_filas) * 100).alias(c) for c in df.columns\n",
        "        ])\n",
        "        resultados[nombre_tabla] = porcentaje_nulos\n",
        "    return resultados\n",
        "\n",
        "tablas = {\n",
        "    \"film\": df_film_final,\n",
        "    \"inventory\": df_inventory_final,\n",
        "    \"rental\": df_rental_final,\n",
        "    \"customer\": df_customer_final,\n",
        "    \"store\": df_store_final\n",
        "}\n",
        "\n",
        "porcentajes_nulos = porcentaje_nulos_por_tabla(tablas)\n",
        "\n",
        "# Mostrar los resultados\n",
        "for nombre_tabla, df_porcentaje in porcentajes_nulos.items():\n",
        "    print(f\"Porcentaje de nulos en la tabla '{nombre_tabla}':\")\n",
        "    df_porcentaje.show()\n",
        "\n",
        "\n",
        "#MER\n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "def generar_df_analitico(df_rental_final, df_customer_final, df_store_final, df_inventory_final, df_film_final):\n",
        "    # Renombramos columnas clave para evitar ambigüedades\n",
        "    df_customer_final = df_customer_final.withColumnRenamed(\"store_id\", \"customer_store_id\")\n",
        "    df_store_final = df_store_final.withColumnRenamed(\"store_id\", \"store_id_store\")\n",
        "    df_inventory_final = df_inventory_final.withColumnRenamed(\"store_id\", \"store_id_inventory\")\n",
        "\n",
        "    return (\n",
        "        df_rental_final\n",
        "        .join(df_customer_final, on=\"customer_id\", how=\"inner\")\n",
        "        .join(df_store_final, col(\"customer_store_id\") == col(\"store_id_store\"), how=\"inner\")\n",
        "        .join(df_inventory_final, on=\"inventory_id\", how=\"inner\")\n",
        "        .join(df_film_final, on=\"film_id\", how=\"inner\")\n",
        "    )\n",
        "\n",
        "df_analitico_final = generar_df_analitico(\n",
        "    df_rental_final,\n",
        "    df_customer_final,\n",
        "    df_store_final,\n",
        "    df_inventory_final,\n",
        "    df_film_final\n",
        ")\n",
        "df_analitico_final.printSchema()\n",
        "df_analitico_final.show(5)\n",
        "\n",
        "#EDA Análisis exploratorio de datos\n",
        "\n",
        "#Analisis univariado\n",
        "#Variables numericas\n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.types import NumericType\n",
        "\n",
        "def analisis_univariado_numerico(df):\n",
        "\n",
        "    columnas_numericas = [f.name for f in df.schema.fields if isinstance(f.dataType, NumericType)]\n",
        "\n",
        "\n",
        "    resumen = df.select(*columnas_numericas).describe()\n",
        "\n",
        "\n",
        "    resumen.show()\n",
        "\n",
        "analisis_univariado_numerico(df_analitico_final)\n",
        "\n",
        "#Variables categoricas\n",
        "\n",
        "from pyspark.sql.functions import col, desc\n",
        "from pyspark.sql import Row\n",
        "\n",
        "def resumen_categoricas(df, top_n=10):\n",
        "    columnas_categoricas = [f.name for f in df.schema.fields\n",
        "                            if f.dataType.simpleString() in ['string', 'boolean']]\n",
        "\n",
        "    columnas_excluir = ['first_name', 'last_name', 'email', 'description']\n",
        "\n",
        "    columnas_filtradas = [c for c in columnas_categoricas if c not in columnas_excluir]\n",
        "\n",
        "    resumen = []\n",
        "\n",
        "    for columna in columnas_filtradas:\n",
        "        count = df.filter(col(columna).isNotNull()).count()\n",
        "        unique = df.select(columna).distinct().count()\n",
        "        fila_top = df.groupBy(columna).count().orderBy(desc(\"count\")).first()\n",
        "\n",
        "        top = fila_top[columna] if fila_top else None\n",
        "        freq = fila_top[\"count\"] if fila_top else None\n",
        "\n",
        "        resumen.append(Row(\n",
        "            columna=columna,\n",
        "            count=count,\n",
        "            unique=unique,\n",
        "            top=top,\n",
        "            freq=freq\n",
        "        ))\n",
        "\n",
        "    return spark.createDataFrame(resumen)\n",
        "\n",
        "resumen_categoricas(df_analitico_final).show()\n",
        "\n",
        "#detectar_outliers y Cuartiles en varialbles numericas\n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.types import NumericType\n",
        "from pyspark.sql import Row\n",
        "\n",
        "def detectar_outliers_iqr_df(df, precision=0.01):\n",
        "    columnas_numericas = [f.name for f in df.schema.fields\n",
        "                          if isinstance(f.dataType, NumericType) and \"id\" not in f.name.lower()]\n",
        "\n",
        "    resumen = []\n",
        "\n",
        "    for columna in columnas_numericas:\n",
        "        try:\n",
        "            q1, q2, q3 = df.approxQuantile(columna, [0.25, 0.5, 0.75], precision)\n",
        "            iqr = q3 - q1\n",
        "            lim_inf = q1 - 1.5 * iqr\n",
        "            lim_sup = q3 + 1.5 * iqr\n",
        "            count_outliers = df.filter((col(columna) < lim_inf) | (col(columna) > lim_sup)).count()\n",
        "\n",
        "            resumen.append(Row(\n",
        "                columna=columna,\n",
        "                Q1=round(q1, 2),\n",
        "                Q2=round(q2, 2),\n",
        "                Q3=round(q3, 2),\n",
        "                Limite_Inferior=round(lim_inf, 2),\n",
        "                Limite_Superior=round(lim_sup, 2),\n",
        "                Outliers=count_outliers\n",
        "            ))\n",
        "        except:\n",
        "            resumen.append(Row(\n",
        "                columna=columna,\n",
        "                Q1=None,\n",
        "                Q2=None,\n",
        "                Q3=None,\n",
        "                Limite_Inferior=None,\n",
        "                Limite_Superior=None,\n",
        "                Outliers=None\n",
        "            ))\n",
        "\n",
        "    df_resumen = spark.createDataFrame(resumen)\n",
        "    df_resumen.select(\"columna\", \"Q1\", \"Q2\", \"Q3\", \"Limite_Inferior\", \"Limite_Superior\", \"Outliers\") \\\n",
        "              .orderBy(\"columna\") \\\n",
        "              .show()\n",
        "\n",
        "    return df_resumen\n",
        "\n",
        "detectar_outliers_iqr_df(df_analitico_final)\n",
        "\n",
        "#Otros analisis del negocio\n",
        "\n",
        "from pyspark.sql.functions import col, count, avg, desc, isnull, datediff, to_date\n",
        "\n",
        "# 1. Número total de películas, clientes, tiendas, alquileres\n",
        "print(\"Número total de películas, clientes, tiendas y alquileres:\")\n",
        "print(\"Películas:\", df_analitico_final.select(\"film_id\").distinct().count())\n",
        "print(\"Clientes:\", df_analitico_final.select(\"customer_id\").distinct().count())\n",
        "print(\"Tiendas:\", df_analitico_final.select(\"store_id_store\").distinct().count())\n",
        "print(\"Alquileres:\", df_analitico_final.select(\"rental_id\").distinct().count())\n",
        "\n",
        "# 2. ¿Cuántos clientes están activos vs. inactivos?\n",
        "print(\"\\nCantidad de clientes activos vs. inactivos:\")\n",
        "df_analitico_final.groupBy(\"active\").count().show()\n",
        "\n",
        "# 3. Número de películas distintas por tienda\n",
        "print(\"\\nNúmero de películas distintas disponibles por tienda:\")\n",
        "df_analitico_final.select(\"store_id_store\", \"film_id\").distinct().groupBy(\"store_id_store\").count().show()\n",
        "\n",
        "# 4. Top 10 clientes con más alquileres\n",
        "print(\"\\nTop 10 clientes con más alquileres:\")\n",
        "df_analitico_final.groupBy(\"customer_id\").count().orderBy(desc(\"count\")).show(10)\n",
        "\n",
        "# 5. Cantidad de alquileres por tienda\n",
        "print(\"\\nCantidad de alquileres por tienda:\")\n",
        "df_analitico_final.groupBy(\"store_id_store\").count().orderBy(\"store_id_store\").show()\n",
        "\n",
        "# 6. Películas más alquiladas\n",
        "print(\"\\nPelículas con mayor número de alquileres:\")\n",
        "df_analitico_final.groupBy(\"title\").count().orderBy(desc(\"count\")).show(10)\n",
        "\n",
        "# 7. Promedio de alquileres por cliente\n",
        "print(\"\\nPromedio de alquileres por cliente:\")\n",
        "alquileres_por_cliente = df_analitico_final.groupBy(\"customer_id\").count()\n",
        "alquileres_por_cliente.select(avg(\"count\").alias(\"avg_rentals\")).show()\n",
        "\n",
        "# 8. Clientes que no devolvieron películas\n",
        "clientes_lost = df_analitico_final.filter(col(\"return_status\") == \"LOST\").select(\"customer_id\").distinct()\n",
        "print(f\"Total de clientes con películas no devueltas: {clientes_lost.count()}\")\n",
        "clientes_lost.show()\n",
        "\n",
        "# 9. Duración promedio de alquileres (días)\n",
        "print(\"\\nDuración promedio de los alquileres en días:\")\n",
        "df_analitico_final = df_analitico_final.withColumn(\"dias_alquiler\", datediff(\"return_date\", \"rental_date\"))\n",
        "df_analitico_final.select(avg(\"dias_alquiler\").alias(\"promedio_dias\")).show()\n",
        "\n",
        "\n",
        "# 10. Valor de perdidas por peliculas no devueltas\n",
        "from pyspark.sql.functions import col, sum as spark_sum\n",
        "\n",
        "print(\"Ingreso potencial perdido por películas con estado LOST:\")\n",
        "\n",
        "ingreso_perdido_lost = df_analitico_final.filter(col(\"return_status\") == \"LOST\") \\\n",
        "    .agg(spark_sum(\"rental_rate\")) \\\n",
        "    .collect()[0][0]\n",
        "\n",
        "print(f\"Perdidas por peliculas no devueltas: ${ingreso_perdido_lost:.2f}\")\n",
        "\n",
        "job.commit()"
      ],
      "metadata": {
        "id": "9b3CDZdIJenN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}